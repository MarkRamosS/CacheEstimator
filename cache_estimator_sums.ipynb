{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3008758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.losses import MeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f76ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "path = '../ce_data/output_512/dataset_ship.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2024b8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(path):\n",
    "    # Read csv\n",
    "    df = pd.read_csv(path)\n",
    "    # Calculate miss ratios in seperate dataframe\n",
    "    miss_ratios = pd.DataFrame()\n",
    "    for i in [\"1024\",\"2048\",\"4096\",\"8192\"]:\n",
    "        miss_ratios[i] = df['misses_'+i] / df['accesses_'+i]\n",
    "        df = df.drop(columns=['misses_'+i, 'accesses_'+i])\n",
    "        if 'l1d_accesses_'+i in df.columns:\n",
    "            df = df.drop(columns=['l1d_accesses_'+i])\n",
    "    # Append filename column\n",
    "    filenames = []\n",
    "    for i in df[\"id\"]:\n",
    "        nmb = i.split('_')[-1]\n",
    "        filenames.append(i[:-len(nmb)-1])\n",
    "    df[\"file_name\"] = filenames\n",
    "    filenames = list(set(filenames))\n",
    "    return df, miss_ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab17d3",
   "metadata": {},
   "source": [
    "There's a chance that reuse distance histograms within files are similar to each other. This may be due to them being sampled from the same files and therefore generated from the same code, so the histograms will be similar. To avoid this interfering with our process, we'll be moving all the lines that are derived from randomly picked files until they add up to 20% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e2c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, miss_ratios, split=0.3):\n",
    "    filenames = list(set(df[\"file_name\"]))\n",
    "    collected_rows = 0\n",
    "    files = []\n",
    "    while len(df) * split > collected_rows:\n",
    "        rf = filenames[random.randint(0,188)]\n",
    "        if rf in files: # No duplicates\n",
    "            continue\n",
    "        files.append(rf)\n",
    "        collected_rows += len(df[df[\"file_name\"] == rf])\n",
    "\n",
    "    train_rds = df[~df[\"file_name\"].isin(files)]\n",
    "    train_mrs = miss_ratios[~df[\"file_name\"].isin(files)]\n",
    "    test_rds = df[df[\"file_name\"].isin(files)]\n",
    "    test_mrs = miss_ratios[df[\"file_name\"].isin(files)]\n",
    "    print(f\"The test set consists of {len(files)} files (out of 189) with a total of {collected_rows} rows (out of {len(df)})\")\n",
    "    train_rds = train_rds.drop(columns=[\"id\", \"file_name\"])\n",
    "    test_rds = test_rds.drop(columns=[\"id\", \"file_name\"])\n",
    "    return train_rds, train_mrs, test_rds, test_mrs\n",
    "#train_rds, train_mrs, test_rds, test_mrs = train_test_split(rds, mrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88e8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(layers=[896,896,1]):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(896, input_dim=896, activation='sigmoid'))\n",
    "    \n",
    "    # Add layers\n",
    "    for i in range(1, len(layers)):\n",
    "        model.add(Dense(layers[i], activation='sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=MeanSquaredError() , optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd7f3fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set consists of 37 files (out of 189) with a total of 1261 rows (out of 6182)\n"
     ]
    }
   ],
   "source": [
    "rds, mrs = read_df('../ce_data/output_512/dataset_ship.csv')\n",
    "train_rds, train_mrs, test_rds, test_mrs = train_test_split(rds, mrs, split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fcd7fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 3588      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,611,012\n",
      "Trainable params: 1,611,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "154/154 [==============================] - 2s 9ms/step - loss: 0.0563\n",
      "Mean squared error for cache size 1024\t: 0.024087349723058114\n",
      "Mean squared error for cache size 2048\t: 0.056733259480368024\n",
      "Mean squared error for cache size 4096\t: 0.0913607822780049\n",
      "Mean squared error for cache size 8192\t: 0.2006422161192692\n",
      "154/154 [==============================] - 1s 8ms/step - loss: 0.0389\n",
      "Mean squared error for cache size 1024\t: 0.021033973145429593\n",
      "Mean squared error for cache size 2048\t: 0.04794679026204141\n",
      "Mean squared error for cache size 4096\t: 0.10566387971066887\n",
      "Mean squared error for cache size 8192\t: 0.19522458349756325\n",
      "154/154 [==============================] - 1s 9ms/step - loss: 0.0284\n",
      "Mean squared error for cache size 1024\t: 0.020356550467558306\n",
      "Mean squared error for cache size 2048\t: 0.04510225404792491\n",
      "Mean squared error for cache size 4096\t: 0.06181546057794366\n",
      "Mean squared error for cache size 8192\t: 0.11012916135387715\n",
      "154/154 [==============================] - 1s 9ms/step - loss: 0.0257\n",
      "Mean squared error for cache size 1024\t: 0.024475627932955296\n",
      "Mean squared error for cache size 2048\t: 0.04722820352867331\n",
      "Mean squared error for cache size 4096\t: 0.07637195424068814\n",
      "Mean squared error for cache size 8192\t: 0.12497013384814358\n",
      "154/154 [==============================] - 1s 9ms/step - loss: 0.0229\n",
      "Mean squared error for cache size 1024\t: 0.018210714114254182\n",
      "Mean squared error for cache size 2048\t: 0.04525420010011109\n",
      "Mean squared error for cache size 4096\t: 0.08762161821443272\n",
      "Mean squared error for cache size 8192\t: 0.14302387511439943\n",
      "154/154 [==============================] - 1s 9ms/step - loss: 0.0203\n",
      "Mean squared error for cache size 1024\t: 0.018779956535481\n",
      "Mean squared error for cache size 2048\t: 0.042751003482857874\n",
      "Mean squared error for cache size 4096\t: 0.07115301395789608\n",
      "Mean squared error for cache size 8192\t: 0.12999644949475272\n",
      "154/154 [==============================] - 1s 10ms/step - loss: 0.0196\n",
      "Mean squared error for cache size 1024\t: 0.016849571519882017\n",
      "Mean squared error for cache size 2048\t: 0.039203627816989786\n",
      "Mean squared error for cache size 4096\t: 0.0710742610293727\n",
      "Mean squared error for cache size 8192\t: 0.1207919362632556\n",
      "154/154 [==============================] - 1s 9ms/step - loss: 0.0170\n",
      "Mean squared error for cache size 1024\t: 0.0181658647995202\n",
      "Mean squared error for cache size 2048\t: 0.03975227939728836\n",
      "Mean squared error for cache size 4096\t: 0.06406609747320062\n",
      "Mean squared error for cache size 8192\t: 0.11906012815526042\n",
      "154/154 [==============================] - 2s 10ms/step - loss: 0.0177\n",
      "Mean squared error for cache size 1024\t: 0.025251175379750388\n",
      "Mean squared error for cache size 2048\t: 0.05236700860235804\n",
      "Mean squared error for cache size 4096\t: 0.08042079732079287\n",
      "Mean squared error for cache size 8192\t: 0.11785914140878277\n",
      "154/154 [==============================] - 1s 10ms/step - loss: 0.0170\n",
      "Mean squared error for cache size 1024\t: 0.016669403313400662\n",
      "Mean squared error for cache size 2048\t: 0.03986128941184678\n",
      "Mean squared error for cache size 4096\t: 0.05878041104132294\n",
      "Mean squared error for cache size 8192\t: 0.10158004540545039\n"
     ]
    }
   ],
   "source": [
    "j = ['1024', '2048', '4096', '8192']\n",
    "m = create_model([896,896,len(j)])\n",
    "m.summary()\n",
    "for i in range(10):\n",
    "    m.fit(train_rds.to_numpy(), train_mrs[j].to_numpy(), epochs=1)\n",
    "#    for i,k in enumerate(j):\n",
    "#        print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "    for i,k in enumerate(j):\n",
    "        print(f'Mean squared error for cache size {k}\\t: {mean_squared_error(m(test_rds.to_numpy())[:,i], test_mrs[k].to_numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6694d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set consists of 59 files (out of 189) with a total of 1287 rows (out of 6183)\n"
     ]
    }
   ],
   "source": [
    "rds, mrs = read_df('../ce_data/output_512/dataset_lru.csv')\n",
    "# print(rds, mrs)\n",
    "train_rds, train_mrs, test_rds, test_mrs = train_test_split(rds, mrs, split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2110edab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 3588      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,611,012\n",
      "Trainable params: 1,611,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "153/153 [==============================] - 2s 9ms/step - loss: 0.0558\n",
      "Mean squared error for cache size 1024\t: 0.02535776785415273\n",
      "Mean squared error for cache size 2048\t: 0.047127148232584605\n",
      "Mean squared error for cache size 4096\t: 0.057039784410750034\n",
      "Mean squared error for cache size 8192\t: 0.08403313409760366\n",
      "153/153 [==============================] - 1s 8ms/step - loss: 0.0339\n",
      "Mean squared error for cache size 1024\t: 0.025319449331812852\n",
      "Mean squared error for cache size 2048\t: 0.043632142641864546\n",
      "Mean squared error for cache size 4096\t: 0.05159009824872934\n",
      "Mean squared error for cache size 8192\t: 0.0848181816039289\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0331\n",
      "Mean squared error for cache size 1024\t: 0.02534774283376257\n",
      "Mean squared error for cache size 2048\t: 0.04193553568910508\n",
      "Mean squared error for cache size 4096\t: 0.051175942238913405\n",
      "Mean squared error for cache size 8192\t: 0.07414325972346016\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0289\n",
      "Mean squared error for cache size 1024\t: 0.025301147013068154\n",
      "Mean squared error for cache size 2048\t: 0.04385379416354359\n",
      "Mean squared error for cache size 4096\t: 0.07340745626180206\n",
      "Mean squared error for cache size 8192\t: 0.1414316452521504\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0294\n",
      "Mean squared error for cache size 1024\t: 0.02531132935133059\n",
      "Mean squared error for cache size 2048\t: 0.04402739430181022\n",
      "Mean squared error for cache size 4096\t: 0.05413801668423861\n",
      "Mean squared error for cache size 8192\t: 0.06653815919715\n",
      "153/153 [==============================] - 1s 9ms/step - loss: 0.0271\n",
      "Mean squared error for cache size 1024\t: 0.025177169745211555\n",
      "Mean squared error for cache size 2048\t: 0.061439144780874505\n",
      "Mean squared error for cache size 4096\t: 0.1134636521144464\n",
      "Mean squared error for cache size 8192\t: 0.09863259541269817\n"
     ]
    }
   ],
   "source": [
    "j = ['1024', '2048', '4096', '8192']\n",
    "m = create_model([896,896,len(j)])\n",
    "m.summary()\n",
    "#for i,k in enumerate(j):\n",
    "#   print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "for i in range(6):\n",
    "    m.fit(train_rds.to_numpy(), train_mrs[j].to_numpy(), epochs=1)\n",
    "#    for i,k in enumerate(j):\n",
    "#        print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "    for i,k in enumerate(j):\n",
    "        print(f'Mean squared error for cache size {k}\\t: {mean_squared_error(m(test_rds.to_numpy())[:,i], test_mrs[k].to_numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e181b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1024      2048      4096      8192\n",
      "0     0.891123  0.795601  0.662782  0.123358\n",
      "1     0.866846  0.765354  0.660919  0.077643\n",
      "2     0.848553  0.727294  0.641544  0.262694\n",
      "3     0.622860  0.225061  0.040571  0.001862\n",
      "4     0.588169  0.200199  0.032464  0.001848\n",
      "...        ...       ...       ...       ...\n",
      "6178  0.785621  0.600917  0.436724  0.297468\n",
      "6179  0.739754  0.549444  0.413295  0.301718\n",
      "6180  0.934980  0.883098  0.810977  0.632974\n",
      "6181  0.949653  0.904684  0.823035  0.558143\n",
      "6182  1.000000  1.000000  1.000000  1.000000\n",
      "\n",
      "[4896 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_mrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3426df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89112255 0.79560149 0.66278225 0.12335835]\n",
      " [0.86684567 0.76535388 0.66091937 0.07764284]\n",
      " [0.84855273 0.72729372 0.64154352 0.26269409]\n",
      " ...\n",
      " [0.93497967 0.88309828 0.81097736 0.6329736 ]\n",
      " [0.94965329 0.90468371 0.8230353  0.5581428 ]\n",
      " [1.         1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(train_mrs.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "294b20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set consists of 22 files (out of 189) with a total of 766 rows (out of 6182)\n"
     ]
    }
   ],
   "source": [
    "rds, mrs = read_df('../ce_data/output_512/dataset_mj.csv')\n",
    "# print(rds, mrs)\n",
    "train_rds, train_mrs, test_rds, test_mrs = train_test_split(rds, mrs, split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb11b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 3588      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,611,012\n",
      "Trainable params: 1,611,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "170/170 [==============================] - 2s 9ms/step - loss: 0.0653\n",
      "Mean squared error for cache size 1024\t: 0.09342126770688144\n",
      "Mean squared error for cache size 2048\t: 0.02668399581363608\n",
      "Mean squared error for cache size 4096\t: 0.03988276473244218\n",
      "Mean squared error for cache size 8192\t: 0.0421484686587087\n",
      "170/170 [==============================] - 1s 9ms/step - loss: 0.0372\n",
      "Mean squared error for cache size 1024\t: 0.025033938031612066\n",
      "Mean squared error for cache size 2048\t: 0.03724689097207383\n",
      "Mean squared error for cache size 4096\t: 0.039012562778898356\n",
      "Mean squared error for cache size 8192\t: 0.02853820103495055\n",
      "170/170 [==============================] - 2s 9ms/step - loss: 0.0254\n",
      "Mean squared error for cache size 1024\t: 0.01197086929919312\n",
      "Mean squared error for cache size 2048\t: 0.02000825663072555\n",
      "Mean squared error for cache size 4096\t: 0.040849152976786464\n",
      "Mean squared error for cache size 8192\t: 0.036170350583818706\n",
      "170/170 [==============================] - 1s 9ms/step - loss: 0.0229\n",
      "Mean squared error for cache size 1024\t: 0.015628522191505994\n",
      "Mean squared error for cache size 2048\t: 0.02225413760662389\n",
      "Mean squared error for cache size 4096\t: 0.032486560347925506\n",
      "Mean squared error for cache size 8192\t: 0.04310711702433924\n",
      "170/170 [==============================] - 2s 9ms/step - loss: 0.0210\n",
      "Mean squared error for cache size 1024\t: 0.013285162008466056\n",
      "Mean squared error for cache size 2048\t: 0.02416717683589404\n",
      "Mean squared error for cache size 4096\t: 0.03437286841462631\n",
      "Mean squared error for cache size 8192\t: 0.03899762970130212\n",
      "170/170 [==============================] - 1s 9ms/step - loss: 0.0206\n",
      "Mean squared error for cache size 1024\t: 0.01320183983102196\n",
      "Mean squared error for cache size 2048\t: 0.021136153664712932\n",
      "Mean squared error for cache size 4096\t: 0.027675561928710533\n",
      "Mean squared error for cache size 8192\t: 0.03204717353045128\n",
      "170/170 [==============================] - 2s 9ms/step - loss: 0.0191\n",
      "Mean squared error for cache size 1024\t: 0.012291885188382033\n",
      "Mean squared error for cache size 2048\t: 0.023222958871558712\n",
      "Mean squared error for cache size 4096\t: 0.03068631164583095\n",
      "Mean squared error for cache size 8192\t: 0.03011127649353479\n",
      "170/170 [==============================] - 1s 9ms/step - loss: 0.0167\n",
      "Mean squared error for cache size 1024\t: 0.010809022049700814\n",
      "Mean squared error for cache size 2048\t: 0.02138809180412754\n",
      "Mean squared error for cache size 4096\t: 0.034117064386774414\n",
      "Mean squared error for cache size 8192\t: 0.031477431125847895\n",
      "170/170 [==============================] - 1s 9ms/step - loss: 0.0156\n",
      "Mean squared error for cache size 1024\t: 0.013291843104380044\n",
      "Mean squared error for cache size 2048\t: 0.019983883318908286\n",
      "Mean squared error for cache size 4096\t: 0.027703322967707122\n",
      "Mean squared error for cache size 8192\t: 0.02934140312413161\n",
      "170/170 [==============================] - 1s 9ms/step - loss: 0.0163\n",
      "Mean squared error for cache size 1024\t: 0.017926763812303452\n",
      "Mean squared error for cache size 2048\t: 0.023622394951868228\n",
      "Mean squared error for cache size 4096\t: 0.027913891193742244\n",
      "Mean squared error for cache size 8192\t: 0.02796756611356121\n"
     ]
    }
   ],
   "source": [
    "j = ['1024', '2048', '4096', '8192']\n",
    "m = create_model([896,896,len(j)])\n",
    "m.summary()\n",
    "#for i,k in enumerate(j):\n",
    "#   print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "for i in range(10):\n",
    "    m.fit(train_rds.to_numpy(), train_mrs[j].to_numpy(), epochs=1)\n",
    "#    for i,k in enumerate(j):\n",
    "#        print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "    for i,k in enumerate(j):\n",
    "        print(f'Mean squared error for cache size {k}\\t: {mean_squared_error(m(test_rds.to_numpy())[:,i], test_mrs[k].to_numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e1f1b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test set consists of 29 files (out of 189) with a total of 683 rows (out of 6181)\n"
     ]
    }
   ],
   "source": [
    "rds, mrs = read_df('../ce_data/output_512/dataset_srrip.csv')\n",
    "# print(rds, mrs)\n",
    "train_rds, train_mrs, test_rds, test_mrs = train_test_split(rds, mrs, split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22c590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 896)               803712    \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 4)                 3588      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,611,012\n",
      "Trainable params: 1,611,012\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "137/172 [======================>.......] - ETA: 0s - loss: 0.0562"
     ]
    }
   ],
   "source": [
    "j = ['1024', '2048', '4096', '8192']\n",
    "m = create_model([896,896,len(j)])\n",
    "m.summary()\n",
    "#for i,k in enumerate(j):\n",
    "#   print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "for i in range(10):\n",
    "    m.fit(train_rds.to_numpy(), train_mrs[j].to_numpy(), epochs=1)\n",
    "#    for i,k in enumerate(j):\n",
    "#        print(mean_squared_error(m(train_rds.to_numpy())[:,i], train_mrs[k].to_numpy()))\n",
    "    for i,k in enumerate(j):\n",
    "        print(f'Mean squared error for cache size {k}\\t: {mean_squared_error(m(test_rds.to_numpy())[:,i], test_mrs[k].to_numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a9a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
